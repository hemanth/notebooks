{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a683ff0b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "### RAG with Gemma and Langchain\n",
    "> Summarize an URL using Gemma \n",
    "\n",
    "ðŸŒ WebBaseLoader: Loads data from a URL.\n",
    "\n",
    "ðŸ¤— HuggingFaceEmbeddings: Generates embeddings using the Hugging Face library.\n",
    "\n",
    "ðŸ—ƒï¸ FAISS: Stores and searches through vectors.\n",
    "\n",
    "ðŸ¤–ðŸ§  google/gemma-2b-it: The Gemma model loaded from Hugging Face.\n",
    "\n",
    "[Hemanth HM](https://h3manth.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6d086-9af3-4517-b114-38e43608dfef",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the deps\n",
    "!pip install langchain langchain_community transformers sentence_transformers faiss-cpu langchainhub -q\n",
    "!pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9faed519-07ce-4b7f-b5d7-db790ef01c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import textwrap\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68996720-e64a-454a-9bda-37e655d4b6b0",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314f381b-9fc5-44b9-a30b-05d7a673b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split documents\n",
    "loader = WebBaseLoader(\"https://h3manth.com\")\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178af238-a501-4c10-a68c-c5fede9c6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b153b6-10b1-40b2-ba89-842fd129f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template  = \"\"\"\n",
    "<start_of_turn>user\n",
    "Answer the question based on your knowledge and the provided context. \n",
    "\n",
    "**Before responding:**\n",
    "\n",
    "* Think carefully about the question and the context.\n",
    "* Write your step-by-step reasoning process.\n",
    "* Explain your answer clearly and concisely.\n",
    "\n",
    "**Question:**\n",
    "\n",
    "{question}\n",
    "\n",
    "**Context:**\n",
    "\n",
    "{context}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23b044cf-8568-42d0-a853-5a70535274a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a1e0048f334354a9690530fa6ca577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76df5213-5853-485f-999c-af002a672533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a0f94e6-5ab0-4aaf-9d4c-2d16fbb583bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM instance \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae30b1c2-9d52-45bc-b099-0aece36d5c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RetrievalQA chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cee95b5-68e7-4b42-915d-294f51a7b1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Sure, here is a summary of the data in 8 points:\n",
       "> \n",
       "> * Hemanth.HM's paws on tech.\n",
       "> * DuckDuckGo community leader.\n",
       "> * Member of Node.js Foundation.\n",
       "> * Google Launchpad Accelerator Mentor.\n",
       "> * TC39er.us podcast host.\n",
       "> * Curates JSfeatures.in.\n",
       "> * Co-hosts ReadSpecWith.us."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask question\n",
    "question = \"Summarize the data in 8 points\"\n",
    "summary = rag_chain.invoke(question)\n",
    "to_markdown(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
